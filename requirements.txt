requests
llama-cpp-python  # optional, required if you want GGUF local inference via `llama_cpp`
# pin versions if desired, e.g. requests==2.31.0
